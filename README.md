"# Gesture-controlled-vehicle" 


Machine Learning Models: I utilized TensorFlow and MediaPipe to develop and train models capable of recognizing hand gestures in real-time.
Image Processing: OpenCV was used for capturing and processing the video feed from the webcam to detect and classify gestures.
Signal Processing:

Developed algorithms to interpret the recognized gestures and convert them into actionable commands for the vehicle.
Hardware Integration:

Arduino Mega: The central microcontroller that interfaced with other hardware components.
Motor Driver (L293D): Controlled the motors responsible for the vehicle's movement.
Bluetooth Module: Enabled wireless communication between the computer (processing the gestures) and the Arduino.
Software Implementation:

Python: Used for implementing the gesture recognition system, processing the webcam input, and sending commands via Bluetooth to the Arduino.
Serial Communication: Ensured reliable data transfer between the computer and the Arduino.
Individual Contributions:
Machine Learning: Developed and trained the gesture recognition models using TensorFlow and MediaPipe.
Software Development: Wrote the Python code for capturing and processing video input, as well as for sending control commands to the Arduino.
Hardware Setup: Configured and programmed the Arduino Mega to receive Bluetooth commands and control the motors accordingly.
Testing and Optimization: Conducted extensive testing to ensure the system's reliability, fine-tuned the gesture recognition accuracy, and optimized the communication between components to minimize latency.

Outcome:
The project successfully demonstrated a functional gesture-controlled autonomous vehicle. It showcased the potential of combining machine learning with hardware to create intuitive and innovative control systems. The system's ability to accurately recognize gestures and control the vehicle in real-time was a significant achievement, highlighting the project's success.








